import json
import os
import time
from typing import Dict, List

import httpx
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama import OllamaLLM

from infra.logs import setup_logging, log_agent_execution
from src.rag.retriever import RAGRetriever

logger = setup_logging()


class Agent:
    def __init__(self):
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.__llm = ollama_base_url

    async def call_llm(self, payload):
        timeout = httpx.Timeout(connect=10.0, read=300.0, write=10.0, pool=10.0)
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                async with client.stream('POST', f"{self.__llm}/api/generate", json=payload) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                data = json.loads(line)
                                if 'response' in data:
                                    yield data['response']
                                if data.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
        except httpx.ReadTimeout:
            raise Exception("Timeout error: The LLM is taking too long to respond.")
        except httpx.HTTPStatusError as e:
            raise Exception(f"HTTP error: {e.response.status_code} - {e.response.text}")
        except Exception as e:
            raise Exception(f"Unexpected error: {e}")


class KnowledgeAgent(Agent):
    def __init__(self, rag: RAGRetriever):
        super().__init__()
        self.retriever = rag
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.llm = OllamaLLM(
            model="llama3.2",
            base_url=ollama_base_url,
            temperature=0.2,
            top_p=0.9,
            repeat_penalty=1.1,
            num_ctx=4096
        )
        self.vectorstore = rag.vectorstore
        self.prompt = ChatPromptTemplate.from_template("""
            Voc√™ √© um assistente virtual especializado EXCLUSIVAMENTE na InfinitePay!

            REGRAS FUNDAMENTAIS:
            1. Primeiro, analise se a pergunta √© sobre InfinitePay, pagamentos, maquininhas, taxas, PIX, cart√µes ou servi√ßos financeiros relacionados
            2. Se a pergunta N√ÉO for sobre esses temas, IGNORE completamente o contexto fornecido e responda EXATAMENTE: "Desculpe, sou especializado somente na InfinitePay e Matem√°tica. Posso te ajudar com perguntas sobre nossos produtos ou servi√ßos e Matem√°tica! üí≥‚ûó"
            3. Se a pergunta FOR sobre InfinitePay, use APENAS as informa√ß√µes do contexto fornecido

            Question: {question}
            Context: {context}

            INSTRU√á√ïES PARA PERGUNTAS SOBRE INFINITEPAY:
            - Use EXCLUSIVAMENTE as informa√ß√µes fornecidas no contexto
            - Se n√£o h√° informa√ß√µes suficientes no contexto, diga: "N√£o tenho essa informa√ß√£o espec√≠fica sobre a InfinitePay no momento. Posso ajudar com outras d√∫vidas sobre nossos produtos e servi√ßos?"
            - Seja completo e detalhado quando as informa√ß√µes est√£o dispon√≠veis
            - Seja sempre simp√°tico e use uma linguagem acess√≠vel
            - Use emojis quando apropriado
            - N√£o mencione "documentos", "fontes" ou "base de conhecimento"

            IMPORTANTE: Mesmo que o contexto contenha informa√ß√µes relevantes, se a pergunta n√£o for sobre InfinitePay, ignore o contexto completamente.

            Analise a pergunta e responda adequadamente:
            """)

        def format_docs(docs):
            """Format documents for the QA chain"""
            return "\n\n".join(doc.page_content for doc in docs)

        self.qa_chain = (
                {
                    "context": self.vectorstore.as_retriever() | format_docs,
                    "question": RunnablePassthrough(),
                }
                | self.prompt
                | self.llm
                | StrOutputParser()
        )

    async def build_llm_payload(self, query: str, stream: bool = True, sources: List[Document] = None) -> Dict:
        """
        Build the payload for the LLM and add personality to the response.
        :param query:
        :param stream:
        :param sources:
        :return:
        """

        if not sources or len(sources) == 0:
            context = "N√£o foram encontrados documentos relevantes na base de conhecimento."
        else:
            context_parts = []
            for i, result in enumerate(sources):
                context_parts.append(
                    f"{result.metadata.get('source_url')} - {result.metadata.get('original_title')}\n\n{result.page_content}\n")
            context = "\n".join(context_parts)

        prompt = f"""Voc√™ √© um assistente virtual amig√°vel e prestativo da InfinitePay! Seu objetivo √© ajudar os clientes com suas d√∫vidas de forma clara, objetiva e acolhedora.

        PERGUNTA DO CLIENTE: {query}

        INFORMA√á√ïES DISPON√çVEIS PARA RESPONDER:
        {context}

        INSTRU√á√ïES CR√çTICAS:
        - As informa√ß√µes acima cont√™m a resposta para a pergunta do cliente
        - Use EXCLUSIVAMENTE essas informa√ß√µes para construir sua resposta e todo o contexto relacionado
        - N√ÉO repita ou parafraseie a pergunta do cliente
        - Se h√° passos numerados ou instru√ß√µes no contexto, organize-os claramente na resposta
        - Seja completo e detalhado quando as informa√ß√µes est√£o dispon√≠veis
        - Seja sempre simp√°tico e use uma linguagem acess√≠vel
        - Use emojis quando apropriado para deixar a conversa mais amig√°vel
        - N√£o mencione "documentos", "fontes" ou "base de conhecimento"
        - APENAS se n√£o houver informa√ß√£o relevante responda: "N√£o tenho essa informa√ß√£o espec√≠fica no momento."
        - Sempre termine oferecendo ajuda adicional

        Baseado nas informa√ß√µes fornecidas acima, responda de forma completa e amig√°vel:"""

        payload = {
            "model": "llama3.2",
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": 0.2,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "num_ctx": 4096
            }
        }

        return payload

    async def process(self, query: str, conversation_id: str = None, user_id: str = None):
        """Process query using LangChain QA chain"""
        start = time.time()

        if not self.vectorstore:
            async for result in self.process_for_stream(query, conversation_id, user_id):
                yield result
            return

        try:
            retriever = self.vectorstore.as_retriever()
            sources = await retriever.ainvoke(query) if hasattr(retriever, 'ainvoke') else retriever.invoke(query)

            if sources:
                source_names = set(source.metadata.get("source", "Documento") for source in sources)

                yield {
                    "type": "sources",
                    "data": {
                        "sources": list(source_names),
                        "documents_found": len(sources)
                    }
                }

                response = await self.qa_chain.ainvoke(query)

                yield {
                    "type": "chunk",
                    "data": {
                        "content": response,
                        "agent": "KnowledgeAgent"
                    }
                }

                execution_time = time.time() - start

                log_agent_execution(
                    logger=logger,
                    agent_name="KnowledgeAgent",
                    conversation_id=conversation_id or "unknown",
                    user_id=user_id or "unknown",
                    execution_time=execution_time,
                    processed_content=f"Query: {query[:100]}... Response: {response[:300]}...",
                    decision=f"QA Chain processed {len(sources)} documents from sources: {', '.join(list(source_names)[:3])}"
                )
            else:
                no_info_response = "Desculpe, n√£o encontrei informa√ß√µes relevantes na minha base de conhecimento sobre essa pergunta. Posso ajudar com outras d√∫vidas sobre InfinitePay?"

                yield {
                    "type": "sources",
                    "data": {
                        "sources": [],
                        "documents_found": 0
                    }
                }

                yield {
                    "type": "chunk",
                    "data": {
                        "content": no_info_response,
                        "agent": "KnowledgeAgent"
                    }
                }

                execution_time = time.time() - start

                log_agent_execution(
                    logger=logger,
                    agent_name="KnowledgeAgent",
                    conversation_id=conversation_id or "unknown",
                    user_id=user_id or "unknown",
                    execution_time=execution_time,
                    processed_content=f"Query: {query[:100]}... No relevant documents found",
                    decision="No relevant documents found in knowledge base"
                )

        except Exception as e:
            logger.error(f"Error in QA chain processing: {e}")
            async for result in self.process_for_stream(query, conversation_id, user_id):
                yield result

    async def process_for_stream(self, query: str, conversation_id: str = None, user_id: str = None):
        """Process query and yield streaming responses"""
        start = time.time()
        full_response = ""

        sources: List[Document] = await self.retriever.search_by_distance(query=query)

        print(f"SOURCES FOUND: {len(sources)}")

        if sources:
            source_names = set(source.metadata.get("source", "Documento") for source in sources)

            yield {
                "type": "sources",
                "data": {
                    "sources": list(source_names),
                    "documents_found": len(sources)
                }
            }

            payload = await self.build_llm_payload(query=query, stream=True, sources=sources)

            async for chunk in self.call_llm(payload):
                full_response += chunk
                yield {
                    "type": "chunk",
                    "data": {
                        "content": chunk,
                        "agent": "KnowledgeAgent"
                    }
                }

            execution_time = time.time() - start

            log_agent_execution(
                logger=logger,
                agent_name="KnowledgeAgent",
                conversation_id=conversation_id or "unknown",
                user_id=user_id or "unknown",
                execution_time=execution_time,
                processed_content=f"Query: {query[:100]}... Response: {full_response[:300]}...",
                decision=f"Retrieved {len(sources)} documents from sources: {', '.join(list(source_names)[:3])}"
            )
        else:
            no_info_response = (
                "Desculpe, n√£o encontrei informa√ß√µes relevantes na minha base de conhecimento sobre essa"
                " pergunta. Posso ajudar com outras d√∫vidas sobre InfinitePay?"
            )

            yield {
                "type": "sources",
                "data": {
                    "sources": [],
                    "documents_found": 0
                }
            }

            yield {
                "type": "chunk",
                "data": {
                    "content": no_info_response,
                    "agent": "KnowledgeAgent"
                }
            }

            execution_time = time.time() - start

            log_agent_execution(
                logger=logger,
                agent_name="KnowledgeAgent",
                conversation_id=conversation_id or "unknown",
                user_id=user_id or "unknown",
                execution_time=execution_time,
                processed_content=f"Query: {query[:100]}... No relevant documents found",
                decision="No relevant documents found in knowledge base"
            )


class MathAgent(Agent):
    def __init__(self):
        super().__init__()

    def build_llm_payload(self, query: str, stream: bool = True) -> Dict:
        prompt = f"""Voc√™ √© um especialista em matem√°tica. Resolva a pergunta abaixo de forma clara e precisa.

        PERGUNTA: {query}

        INSTRU√á√ïES:
        - Calcule corretamente e mostre o resultado final
        - Explique brevemente os passos principais
        - Use s√≠mbolos matem√°ticos quando necess√°rio
        - Escreva em portugu√™s brasileiro
        - Destaque a resposta final claramente

        RESPOSTA:"""

        payload = {
            "model": "llama3.2",
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": 0.1,
                "top_p": 0.9
            }
        }

        return payload

    async def process(self, query: str, conversation_id: str = None, user_id: str = None):
        """Process query and yield streaming responses"""
        start = time.time()
        full_response = ""

        yield {
            "type": "sources",
            "data": {
                "sources": ["C√°lculos Matem√°ticos"],
                "processing": "resolvendo problema matem√°tico"
            }
        }

        payload = self.build_llm_payload(query=query, stream=True)

        async for chunk in self.call_llm(payload):
            full_response += chunk
            yield {
                "type": "chunk",
                "data": {
                    "content": chunk,
                    "agent": "MathAgent"
                }
            }

        execution_time = time.time() - start

        log_agent_execution(
            logger=logger,
            agent_name="MathAgent",
            conversation_id=conversation_id or "unknown",
            user_id=user_id or "unknown",
            execution_time=execution_time,
            processed_content=f"Query: {query[:100]}... Response: {full_response[:300]}...",
            decision="Mathematical calculation processed"
        )
